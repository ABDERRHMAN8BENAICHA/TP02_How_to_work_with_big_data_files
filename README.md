# ğŸ“Š TP02 -- Working with Large CSV Files (6GB+) in Python

## ğŸ“Œ Project Overview

This project demonstrates how to efficiently process large CSV files
(\>5GB) using different Big Data techniques in Python.

Dataset used in this experiment:

-   **File name:** `big_5gb.csv`
-   **File size:** 6.00 GB

The objective is to compare performance and storage efficiency using
three different approaches.

------------------------------------------------------------------------

## ğŸ— Project Structure

    TP02_How_to_work_with_big_data_files/
    â”‚
    â”œâ”€â”€ big_5gb.csv
    â”œâ”€â”€ dist/
    â”‚   â””â”€â”€ results.txt
    â”‚
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ pandas_chunk.py
    â”‚   â”œâ”€â”€ dask_method.py
    â”‚   â”œâ”€â”€ compression_method.py
    â”‚   â””â”€â”€ main.py
    â”‚
    â”œâ”€â”€ cpu_performance.png
    â”‚
    â””â”€â”€ README.md

------------------------------------------------------------------------

## âš™ï¸ Technologies Used

-   Python 3.12
-   Pandas
-   Dask
-   gzip

------------------------------------------------------------------------

## ğŸš€ Methods Implemented

### 1ï¸âƒ£ Pandas Chunking

Reads the large CSV file in smaller partitions using:

pd.read_csv(file, chunksize=100000)

âœ” Reduces memory usage\
âœ” Stable\
âŒ Sequential processing (slower)

------------------------------------------------------------------------

### 2ï¸âƒ£ Dask

Uses parallel processing to improve performance:

dd.read_csv(file)

âœ” Fastest method\
âœ” Designed for large datasets\
âœ” Scalable

------------------------------------------------------------------------

### 3ï¸âƒ£ Compression (gzip)

-   Original size: **6.00 GB**
-   Compressed size: **1.39 GB**
-   Storage reduction: \~77%

âœ” Excellent storage optimization\
âŒ Slower read time due to decompression

------------------------------------------------------------------------

## ğŸ“Š Experimental Results

  Method            Execution Time (seconds)   File Size (GB)
  ----------------- -------------------------- ----------------
  Pandas Chunk      70.70 s                    6.00 GB
  Dask              50.70 s                    6.00 GB
  Compressed Read   94.39 s                    1.39 GB

------------------------------------------------------------------------

## ğŸ“ˆ Performance Analysis

-   **Fastest Method:** Dask (50.70 seconds)
-   **Best Storage Optimization:** Compression (77% reduction)
-   **Balanced Approach:** Pandas chunking

Dask performed better due to parallel computation, while compression
significantly reduced disk usage at the cost of increased processing
time.

------------------------------------------------------------------------
## CPU Performance Visualization

The following chart compares CPU execution time between the three methods:

![CPU Performance Comparison](cpu_performance.jpg)

Observations:
- Dask achieved the fastest execution time.
- Pandas chunking performed moderately.
- Compressed reading required more CPU time due to decompression.


------------------------------------------------------------------------

## ğŸ§  Key Learnings

-   Large CSV files cannot be safely loaded entirely into memory.
-   Chunking improves memory management.
-   Parallel processing significantly improves performance.
-   Compression reduces storage but increases CPU usage.

------------------------------------------------------------------------

## â–¶ï¸ How to Run

Activate virtual environment:

source .venv/bin/activate

Install dependencies:

pip install pandas dask pyarrow

Run the project:

python src/main.py

------------------------------------------------------------------------

## ğŸ¯ Conclusion

This TP demonstrates practical techniques for handling large-scale data
efficiently using Python.

-   Use **Dask** for performance-critical workloads.
-   Use **Compression** for storage optimization.
-   Use **Pandas chunking** for controlled memory usage.
